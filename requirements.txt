# FastAPI and server
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6

# LangChain
# Compatible with LlamaCpp
langchain>=0.1.0
langchain-community>=0.0.10

# Vector store and embeddings
faiss-cpu==1.7.4
sentence-transformers>=2.2.0

# GPU support for local LLM
# Note: Installed with CMAKE_ARGS="-DGGML_CUDA=on"
llama-cpp-python>=0.2.20

# Utilities
scipy>=1.11.0
numpy==1.24.4
pypdf==4.0.1
python-dotenv==1.0.0
pydantic>=2.0
pydantic-settings>=2.0
httpx==0.26.0

# Frontend
streamlit>=1.28.0
requests>=2.31.0
tqdm

